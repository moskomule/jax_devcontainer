{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Flax's annotated MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=10)(x)\n",
    "        x = nn.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "def cross_entropy_loss(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "    onehot = jax.nn.one_hot(labels, num_classes=10)\n",
    "    return -jnp.mean(jnp.sum(onehot * logits, axis=-1))\n",
    "\n",
    "def compute_metrics(logits: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "    loss = cross_entropy_loss(logits, labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "\n",
    "    return {'loss': loss, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    ds_builder = tfds.builder('mnist')\n",
    "    ds_builder.download_and_prepare()\n",
    "    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "    train_ds['image'] = jnp.float32(train_ds['image']) / 255.0\n",
    "    test_ds['image'] = jnp.float32(test_ds['image']) / 255.0\n",
    "    return train_ds, test_ds\n",
    "\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    cnn = CNN()\n",
    "    params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    return train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = CNN().apply({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits, batch['label'])\n",
    "        return loss, logits\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits, batch['label'])\n",
    "    return state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    logits = CNN().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits, labels=batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch*batch_size]\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    batch_metrics = []\n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "    \n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]\n",
    "    }\n",
    "\n",
    "    print(f\"train epoch: {epoch} loss: {epoch_metrics_np['loss']:.4f} accuracy: {epoch_metrics_np['accuracy']:.4f}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(params, test_ds):\n",
    "    metrics = eval_step(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "state = create_train_state(init_rng, learning_rate, momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 0 loss: 0.1334 accuracy: 0.9592\n",
      ">>> loss:0.0614 accuracy: 0.9796\n",
      "train epoch: 1 loss: 0.0481 accuracy: 0.9853\n",
      ">>> loss:0.0540 accuracy: 0.9842\n",
      "train epoch: 2 loss: 0.0336 accuracy: 0.9898\n",
      ">>> loss:0.0311 accuracy: 0.9900\n",
      "train epoch: 3 loss: 0.0246 accuracy: 0.9921\n",
      ">>> loss:0.0360 accuracy: 0.9912\n",
      "train epoch: 4 loss: 0.0212 accuracy: 0.9932\n",
      ">>> loss:0.0340 accuracy: 0.9905\n",
      "train epoch: 5 loss: 0.0174 accuracy: 0.9948\n",
      ">>> loss:0.0286 accuracy: 0.9915\n",
      "train epoch: 6 loss: 0.0114 accuracy: 0.9965\n",
      ">>> loss:0.0413 accuracy: 0.9888\n",
      "train epoch: 7 loss: 0.0100 accuracy: 0.9971\n",
      ">>> loss:0.0462 accuracy: 0.9890\n",
      "train epoch: 8 loss: 0.0096 accuracy: 0.9971\n",
      ">>> loss:0.0381 accuracy: 0.9897\n",
      "train epoch: 9 loss: 0.0083 accuracy: 0.9974\n",
      ">>> loss:0.0366 accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    state = train_epoch(state, train_ds, batch_size, epoch, input_rng)\n",
    "    test_loss, test_accuracy = eval_model(state.params, test_ds)\n",
    "    print(f\">>> loss:{test_loss:.4f} accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
